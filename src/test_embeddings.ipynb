{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "candidate_texts = [\n",
    "    \"The grasp is on the handle of the mug, from the side.\",\n",
    "    \"Grasp is from above, on the rim of the mug. The grasp is perpendicular to the opening of the mug.\",\n",
    "    \"The grasp is on the body of the mug, from the side, oriented parallel to the base of the mug.\"\n",
    "]\n",
    "\n",
    "proposal_text = \"The mug should be grasped around its cylindrical body, approximately halfway up from the base to the rim. Orient the parallel gripper fingers horizontally, adjusting them so that both fingers are equidistant from either side of the mug, ensuring a firm hold. Avoid the area near the handle to maintain a balanced grip.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "def encode_clip(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    rets = []\n",
    "    with torch.no_grad():\n",
    "        clip_inputs = clip_processor(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        rets.append(clip_model.get_text_features(**clip_inputs))\n",
    "    return torch.stack(rets)\n",
    "\n",
    "start = time.perf_counter()\n",
    "candidates = encode_clip(candidate_texts)\n",
    "proposal = encode_clip(proposal_text)\n",
    "elapsed = time.perf_counter() - start\n",
    "scores = (candidates @ proposal.reshape(-1, 1)) / (torch.norm(candidates, dim=-1, keepdim=True) * torch.norm(proposal))\n",
    "scores = scores.cpu().numpy().flatten()\n",
    "results[\"clip\"] = {\"scores\": scores, \"elapsed\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "mpnet = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "def encode_mpnet(text):\n",
    "    with torch.no_grad():\n",
    "        return mpnet.encode(text, convert_to_tensor=True)\n",
    "\n",
    "start = time.perf_counter()\n",
    "candidates = encode_mpnet(candidate_texts)\n",
    "proposal = encode_mpnet(proposal_text)\n",
    "elapsed = time.perf_counter() - start\n",
    "scores = (candidates @ proposal.reshape(-1, 1)) / (torch.norm(candidates, dim=-1, keepdim=True) * torch.norm(proposal))\n",
    "scores = scores.cpu().numpy().flatten()\n",
    "results[\"mpnet\"] = {\"scores\": scores, \"elapsed\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "crossencoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "start = time.perf_counter()\n",
    "ranking = crossencoder.rank(proposal_text, candidate_texts)\n",
    "elapsed = time.perf_counter() - start\n",
    "ranked_scores = [ranking[\"score\"] for ranking in ranking]\n",
    "ranked_ids = [ranking[\"corpus_id\"] for ranking in ranking]\n",
    "scores = np.zeros(len(candidate_texts))\n",
    "for i, id in enumerate(ranked_ids):\n",
    "    scores[id] = ranked_scores[i]\n",
    "results[\"crossencoder\"] = {\"scores\": scores, \"elapsed\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90efdf4303a468eacb0b6140c169089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nfs2.prior/abhayd/envs/torch/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "query_prefix = \"Instruct: Given a description of a grasp, retrieve grasp descriptions that describe similar grasps on similar objects\\nQuery: \"\n",
    "\n",
    "if \"nv_embed\" not in globals():\n",
    "    nv_embed = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "    nv_embed.max_seq_length = 32768\n",
    "    nv_embed.tokenizer.padding_side=\"right\"\n",
    "\n",
    "def add_eos(input_examples):\n",
    "    input_examples = [input_example + nv_embed.tokenizer.eos_token for input_example in input_examples]\n",
    "    return input_examples\n",
    "\n",
    "# get the embeddings\n",
    "batch_size = 2\n",
    "with torch.no_grad():\n",
    "    start = time.perf_counter()\n",
    "    query_embeddings = nv_embed.encode(add_eos([proposal_text]), batch_size=batch_size, prompt=query_prefix, normalize_embeddings=True)\n",
    "    passage_embeddings = nv_embed.encode(add_eos(candidate_texts), batch_size=batch_size, normalize_embeddings=True)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "scores = (query_embeddings @ passage_embeddings.T).flatten()\n",
    "results[\"nv-embed\"] = {\"scores\": scores, \"elapsed\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposal: The mug should be grasped around its cylindrical body, approximately halfway up from the base to the rim. Orient the parallel gripper fingers horizontally, adjusting them so that both fingers are equidistant from either side of the mug, ensuring a firm hold. Avoid the area near the handle to maintain a balanced grip.\n",
      "Candidates:\n",
      "\t1: The grasp is on the handle of the mug, from the side.\n",
      "\t2: Grasp is from above, on the rim of the mug. The grasp is perpendicular to the opening of the mug.\n",
      "\t3: The grasp is on the body of the mug, from the side, oriented parallel to the base of the mug.\n",
      "\n",
      "clip\n",
      "\tScores: [0.8430011 0.8193814 0.8449857]\n",
      "\tRanking: [3 1 2]\n",
      "\tElapsed: 0.05918162688612938\n",
      "mpnet\n",
      "\tScores: [0.7771651  0.74865735 0.8147641 ]\n",
      "\tRanking: [3 1 2]\n",
      "\tElapsed: 0.810763094574213\n",
      "crossencoder\n",
      "\tScores: [0.57904649 0.60282588 0.72182417]\n",
      "\tRanking: [3 2 1]\n",
      "\tElapsed: 0.02026798017323017\n",
      "nv-embed\n",
      "\tScores: [0.5280144  0.53775704 0.6598767 ]\n",
      "\tRanking: [3 2 1]\n",
      "\tElapsed: 0.3232739083468914\n"
     ]
    }
   ],
   "source": [
    "print(f\"Proposal: {proposal_text}\")\n",
    "print(\"Candidates:\")\n",
    "for i, text in enumerate(candidate_texts):\n",
    "    print(f\"\\t{i+1}: {text}\")\n",
    "print()\n",
    "\n",
    "for model, result in results.items():\n",
    "    print(model)\n",
    "    scores = result[\"scores\"]\n",
    "    print(f\"\\tScores: {scores}\")\n",
    "    print(f\"\\tRanking: {np.argsort(scores)[::-1]+1}\")\n",
    "    print(f\"\\tElapsed: {result['elapsed']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
