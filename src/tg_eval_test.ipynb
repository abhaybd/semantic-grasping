{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from semantic_grasping_datagen.eval.generate_taskgrasp_eval import TaskGraspScanLibrary, img_to_pc\n",
    "\n",
    "from scorer import load_scorer\n",
    "from utils import backproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANS_DIR = \"../data/taskgrasp/scans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TaskGraspScanLibrary(SCANS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_scorer = load_scorer(\"01JQW5E267HTPPQFPJ2V23W8S3\", ckpt=12500, map_location=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, DepthProImageProcessorFast, DepthProForDepthEstimation\n",
    "import torch\n",
    "\n",
    "if \"depth_anything\" not in globals():\n",
    "    depth_anything = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "\n",
    "if \"depth_pro_model\" not in globals():\n",
    "    depth_pro_processor = DepthProImageProcessorFast.from_pretrained(\"apple/DepthPro-hf\")\n",
    "    depth_pro_model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\").cuda()\n",
    "\n",
    "def complete_depth(rgb: Image.Image, depth: np.ndarray) -> np.ndarray:\n",
    "    depth_relative = np.array(depth_anything(rgb)[\"depth\"])\n",
    "    mask = depth > 0\n",
    "    masked_metric_depth = depth[mask].astype(np.float32)\n",
    "    masked_relative_depth = depth_relative[mask].astype(np.float32)\n",
    "\n",
    "    # Find relation between relative and metric depth\n",
    "    A = np.vstack([masked_relative_depth, np.ones(len(masked_relative_depth))]).T\n",
    "    scale, shift = np.linalg.lstsq(A, masked_metric_depth, rcond=None)[0]\n",
    "\n",
    "    depth_pred = scale * depth_relative + shift\n",
    "    return depth_pred\n",
    "\n",
    "def complete_depth_dp(rgb: Image.Image, depth: np.ndarray) -> np.ndarray:\n",
    "    inputs = depth_pro_processor(rgb, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = depth_pro_model(**inputs)\n",
    "    processed_outputs = depth_pro_processor.post_process_depth_estimation(outputs, target_sizes=[depth.shape])\n",
    "    depth_pred = processed_outputs[0][\"predicted_depth\"].cpu().numpy()\n",
    "\n",
    "    depth_pred_masked = depth_pred[depth > 0]\n",
    "    depth_masked = depth[depth > 0]\n",
    "\n",
    "    A = np.vstack([depth_pred_masked, np.ones(len(depth_pred_masked))]).T\n",
    "    scale, shift = np.linalg.lstsq(A, depth_masked, rcond=None)[0]\n",
    "\n",
    "    depth_pred = scale * depth_pred + shift\n",
    "    print(scale, shift)\n",
    "    return depth_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get(\"236_mug\", 2)\n",
    "\n",
    "rgb = data[\"rgb\"]\n",
    "depth = data[\"depth\"]\n",
    "pc = data[\"fused_pc\"]\n",
    "grasps = data[\"registered_grasps\"]\n",
    "cam_K = data[\"cam_params\"]\n",
    "\n",
    "rgb.show()\n",
    "\n",
    "FAR_CLIP = 0.8\n",
    "\n",
    "depth[depth > FAR_CLIP] = 0\n",
    "completed_depth = complete_depth_dp(rgb, depth)\n",
    "\n",
    "xyz = backproject(cam_K, completed_depth)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "cmap = matplotlib.colormaps.get_cmap(\"viridis\")\n",
    "cmap.set_bad(color=\"black\")\n",
    "vmin = np.minimum(np.nanmin(depth), np.nanmin(completed_depth))\n",
    "vmax = np.maximum(np.nanmax(depth), np.nanmax(completed_depth))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "depth_viz = depth.copy()\n",
    "depth_viz[depth == 0] = np.nan\n",
    "print(np.nanmin(depth_viz), np.nanmax(depth_viz))\n",
    "plt.imshow(depth_viz, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original Depth\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(completed_depth, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "plt.axis(\"off\") \n",
    "plt.title(\"Completed Depth\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "cmap2 = matplotlib.colormaps.get_cmap(\"coolwarm\")\n",
    "cmap2.set_bad(color=\"black\")\n",
    "plt.imshow(completed_depth - depth_viz, cmap=cmap2)\n",
    "plt.colorbar()\n",
    "plt.title(\"Difference (m)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "text = \"The grasp is on the mug. The grasp is on the handle of the mug. It is oriented vertically, with the fingers grasping the sides of the body from above.\"\n",
    "# text = \"The grasp is on the pan. The grasp is on the handle of the pan. It is oriented vertically, with the fingers grasping the sides of the handle.\"\n",
    "# text = \"The grasp is on the pan. The grasp is on the rim of the pan. It is oriented vertically, with the fingers grasping the inside and outside of the pan.\"\n",
    "\n",
    "trf = np.eye(4)\n",
    "trf[[1,2]] = -trf[[1,2]]\n",
    "trf_grasps = trf[None] @ grasps\n",
    "pred = grasp_scorer.score_grasps(np.eye(4), rgb, xyz, trf_grasps, text)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from acronym_tools import create_gripper_marker\n",
    "\n",
    "scene = trimesh.Scene()\n",
    "scene_pc = img_to_pc(np.asarray(rgb), depth, cam_K, depth < FAR_CLIP)\n",
    "# scene_pc[:, 3:] = [[255, 0, 0]]\n",
    "completed_pc = img_to_pc(np.asarray(rgb), completed_depth, cam_K, completed_depth < FAR_CLIP)\n",
    "scene_pc_obj = trimesh.PointCloud(scene_pc[:,:3], scene_pc[:,3:].astype(np.uint8))\n",
    "completed_pc_obj = trimesh.PointCloud(completed_pc[:,:3], completed_pc[:,3:].astype(np.uint8))\n",
    "scene.add_geometry(scene_pc_obj)\n",
    "# scene.add_geometry(completed_pc_obj)\n",
    "\n",
    "for p, grasp in zip(pred, grasps):\n",
    "    val = np.interp(p, [0.6, 0.875], [0, 255]).round().astype(np.uint8)\n",
    "    gripper: trimesh.Trimesh = create_gripper_marker([255-val, val, 0])\n",
    "    gripper.apply_transform(grasp)\n",
    "    scene.add_geometry(gripper)\n",
    "\n",
    "# best_idx = np.argmax(pred)\n",
    "# for i, grasp in enumerate(grasps):\n",
    "#     color = [0,255,0] if i == best_idx else [255,0,0]\n",
    "#     gripper: trimesh.Trimesh = create_gripper_marker(color)\n",
    "#     gripper.apply_transform(grasp)\n",
    "#     scene.add_geometry(gripper)\n",
    "\n",
    "axes = trimesh.creation.axis(origin_size=0.025)\n",
    "# scene.add_geometry(axes)\n",
    "\n",
    "scene.lights\n",
    "\n",
    "scene.show(line_settings={'point_size':100}, height=720)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
